{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Q4PrfSyhRv"
      },
      "source": [
        "# **Regularized Logistic Regression**\n",
        "\n",
        "In this notebook, we will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly. \n",
        "\n",
        "# **Problem Statement**\n",
        "\n",
        "Suppose I am the product manager of the factory and we have the test results for some microchips on two different tests. \n",
        "- From these two tests, we would like to determine whether the microchips should be accepted or rejected. \n",
        "- To help us make the decision, we have a dataset of test results on past microchips, from which we can build a logistic regression model.\n",
        "\n",
        "# **Loading and visualizing the data**\n",
        "\n",
        "Let's start by loading the dataset for this task and visualizing it. \n",
        "\n",
        "- The `load_dataset()` function shown below loads the data into variables `X_train` and `y_train`\n",
        "  - `X_train` contains the test results for the microchips from two tests\n",
        "  - `y_train` contains the results of the QA  \n",
        "      - `y_train = 1` if the microchip was accepted \n",
        "      - `y_train = 0` if the microchip was rejected \n",
        "  - Both `X_train` and `y_train` are numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "from google.colab import drive\n",
        "import matplotlib.image as mpimg \n",
        "drive.mount('/content/drive')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18pq1TFC1swP",
        "outputId": "8ede22d6-2b61-47d9-80f8-0aa8f292b5a7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filename):\n",
        "    data = np.loadtxt(filename, delimiter=',')\n",
        "    X = data[:,:2]\n",
        "    y = data[:,2]\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "Xn3RXM_V165n"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "vTsSf93ryhRv"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "path_testing = \"/content/drive/MyDrive/Code/logistic-regression-models/ex2data2.txt\"\n",
        "X_train, y_train = load_data(path_testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mccLRN5yhRv"
      },
      "source": [
        "# **View the variables**\n",
        "\n",
        "The code below prints the first five values of `X_train` and `y_train` and the type of the variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3sp6GMOyhRw",
        "outputId": "801190a4-60aa-4e94-be2e-bd6aeb4adaaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: [[ 0.051267  0.69956 ]\n",
            " [-0.092742  0.68494 ]\n",
            " [-0.21371   0.69225 ]\n",
            " [-0.375     0.50219 ]\n",
            " [-0.51325   0.46564 ]]\n",
            "Type of X_train: <class 'numpy.ndarray'>\n",
            "y_train: [1. 1. 1. 1. 1.]\n",
            "Type of y_train: <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# print X_train\n",
        "print(\"X_train:\", X_train[:5])\n",
        "print(\"Type of X_train:\",type(X_train))\n",
        "\n",
        "# print y_train\n",
        "print(\"y_train:\", y_train[:5])\n",
        "print(\"Type of y_train:\",type(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4LKOyRhyhRw"
      },
      "source": [
        "#### **Check the dimensions of variables**\n",
        "\n",
        "Another useful way to get familiar with the data is to view its dimensions. Let's print the shape of `X_train` and `y_train` and see how many training examples we have in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5qrg7CGyhRx",
        "outputId": "dc5b72b8-99ee-4b5a-c607-48ac3f9462fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of X_train is: (118, 2)\n",
            "The shape of y_train is: (118,)\n",
            "We have m = 118 training examples\n"
          ]
        }
      ],
      "source": [
        "print ('The shape of X_train is: ' + str(X_train.shape))\n",
        "print ('The shape of y_train is: ' + str(y_train.shape))\n",
        "print ('We have m = %d training examples' % (len(y_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD5WNTsMyhRx"
      },
      "source": [
        "#### **Visualize the data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(X, y, pos_label=\"y=1\", neg_label=\"y=0\"):\n",
        "    positive = y == 1\n",
        "    negative = y == 0\n",
        "    \n",
        "    # Plot examples\n",
        "    plt.plot(X[positive, 0], X[positive, 1], 'k+', label=pos_label)\n",
        "    plt.plot(X[negative, 0], X[negative, 1], 'yo', label=neg_label)"
      ],
      "metadata": {
        "id": "_fXZsAdd55Ni"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "TPuhBgpUyhR0",
        "outputId": "bdf779dc-2265-4685-8e67-a599c74d1aae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEGCAYAAABLgMOSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhU9Zno8e/LYqNGEbAvEpEtQeMCtGzaaiIGBUyegI4k4oqOBolB5443jvhwM3RMSHQ0TxyNcZngNjKA4UbFLG5I66jINkFFCIItRBgFBkTBpYXu9/5xTuPp6qrqWs5a5/08Tz1ddZY6vzpddd7z20VVMcYYY4rVIeoEGGOMSSYLIMYYY0piAcQYY0xJLIAYY4wpiQUQY4wxJekUdQLCdPjhh2u/fv2iToYxxiTKypUr/0dVqzOXpyqA9OvXjxUrVkSdDGOMSRQR2ZRtuRVhGWOMKYkFEGOMMSWxAGKMMaYkqaoDMcZUpr1797J582Y+++yzqJOSaF26dKF379507ty5oO0tgBhjEm/z5s0ccsgh9OvXDxGJOjmJpKrs2LGDzZs3079//4L2sSIsk2pbt85hyZJ+1Nd3YMmSfmzdOifqJJkSfPbZZ/To0cOCRxlEhB49ehSVi7MciEmtrVvnsG7dFJqbPwGgsXET69ZNAaBnz4uiTJopgQWP8hV7Di0HYlKroWHG/uDRorn5ExoaZkSUImOSxQKISa3Gxr8VtTzprLgueI8//jgiwl//+tdQjvfzn/+86H0efPBBpk2b5svxLYCY1Kqq6lPU8iRrKa5rbNwE6P7iurQHkbq6Ol/fb+7cuZx22mnMnTvX1/fNpZQA4icLICa1BgyYRYcOB7Va1qHDQQwYMCuiFAXHiuuy+8lPfuLbe+3Zs4eXXnqJ2bNnM2/ePACampr40Y9+xAknnMDgwYO58847AVi+fDmnnHIKQ4YMYeTIkezevZumpiauv/56RowYweDBg7n33nsBqK+v5xvf+Abf/va3OeaYY5g6dSrNzc1Mnz6dTz/9lJqaGi66yKmze+SRRxg5ciQ1NTVcddVVNDU1AfDAAw9w9NFHM3LkSF5++WXfPrNVopvUaqkob2iYQWPj36iq6sOAAbMqsgI9bcV1UXjiiScYN24cRx99ND169GDlypUsW7aMjRs3smrVKjp16sTOnTv5/PPPOf/885k/fz4jRozgo48+4sADD2T27Nl07dqV5cuX09jYyKmnnsqYMWMAWLZsGWvWrKFv376MGzeO3//+99x88838+te/ZtWqVQCsXbuW+fPn8/LLL9O5c2euvvpq5syZw1lnncXMmTNZuXIlXbt25YwzzuDEE0/05TNbDsSkWs+eF1Fbu5FRo5qprd1YkcED0lVc1566ujpEZH+Lo5bn5RZnzZ07l0mTJgEwadIk5s6dy3PPPcdVV11Fp07OvXr37t1Zt24dvXr1YsSIEQAceuihdOrUiWeeeYaHH36YmpoaTjrpJHbs2MH69esBGDlyJAMGDKBjx45ccMEFvPTSS22Ov2jRIlauXMmIESOoqalh0aJFNDQ0sHTpUkaNGkV1dTUHHHAA559/flmf08tyIMakwIABs1o1WYbKLa5rT11d3f5gISKoatnvuXPnTp5//nneeOMNRISmpiZEZH+QKISqcueddzJ27NhWy+vr69s0r83W3FZVmTx5Mr/4xS9aLX/88ceL+CTFsRyIMSnQs+dFHHPMfVRV9QWEqqq+HHPMfRWb4wrbggULuOSSS9i0aRMbN27k3XffpX///gwZMoR7772Xffv2AU6gOeaYY3jvvfdYvnw5ALt372bfvn2MHTuWu+++m7179wLw1ltv8fHHHwNOEdY777xDc3Mz8+fP57TTTgOgc+fO+7cfPXo0CxYsYNu2bfuPtWnTJk466SReeOEFduzYwd69e/nd737n2+e2HIgxKdGz50UWMDLMnDnTl/eZO3cuN9xwQ6tl5513HmvXrqVPnz4MHjyYzp078/3vf59p06Yxf/58rrnmGj799FMOPPBAnnvuOa688ko2btzI0KFDUVWqq6v35x5GjBjBtGnT2LBhA2eccQbnnnsuAFOmTGHw4MEMHTqUOXPm8LOf/YwxY8bQ3NxM586dueuuuzj55JOpq6ujtraWww47jJqaGl8+M4D4kX1LiuHDh6tNKGVM5Vm7di3HHnts1MkIRH19Pbfddht/+MMfQjletnMpIitVdXjmtpEWYYnI/SKyTURW51gvInKHiGwQkddFZKhn3WQRWe8+JoeX6sphHcuKZ+fMmC9EXQfyIDAuz/qzgYHuYwpwN4CIdAdmAicBI4GZItIt0JRWmLR2LCsnAKT1nJlojRo1KrTcR7EiDSCq+iKwM88mE4CH1fEqcJiI9ALGAs+q6k5V/QB4lvyByGRIY8eycgNAGs+ZMflEnQNpz5HAu57Xm91luZa3ISJTRGSFiKzYvn17YAlNmjR2LCs3AKTxnBmTT9wDSNlU9T5VHa6qw6urq6NOTmzEtWOZ32MTeZUbAOJ6zoyJStwDyBbgKM/r3u6yXMtNgeI6DpSfYxNlKjcAxPWcGROVuAeQhcClbmusk4EPVfU94GlgjIh0cyvPx7jLTIHS2LGs3ACQxnNmCtexY0dqamo44YQT+M53vsOuXbvybn/PPffw8MMPF32cXbt28Zvf/Kbo/erq6rjtttuK3i+fqJvxzgWWAMeIyGYRuUJEporIVHeTPwENwAbg34CrAVR1J/BTYLn7uMldZooQl3GgghqbKJMfASCqc2bNh/0VxPk88MADWbVqFatXr6Z79+7cddddebefOnUql156adHHKTWABCHqVlgXqGovVe2sqr1Vdbaq3qOq97jrVVV/qKpfUdVBqrrCs+/9qvpV9/FAdJ8i3fz4IdbV1aGq+8ckankeRH1IWAHAzwuUNR/2Vxjns7a2li1bnFL1t99+m3HjxjFs2DC+/vWv759sypsjyLXN1q1bOffccxkyZAhDhgzhlVdeYfr06bz99tvU1NRw/fXXA3DrrbfuHwbe27t+1qxZHH300Zx22mmsW7fOt8/XwoYyMSWzOcWz8/u85Gs9lubzXKqgz2dTUxOLFi3iiiuuAJzhRu655x4GDhzI0qVLufrqq3n++edb7ZNrm2uvvZbTTz+dxx57jKamJvbs2cPNN9/M6tWr9w/j/swzz7B+/XqWLVuGqjJ+/HhefPFFDj74YObNm8eqVavYt28fQ4cOZdiwYWV/Pi8LICmzdesc3+a/COKH6NfYRFHy+7xY82F/BXU+WyZ32rJlC8ceeyxnnXUWe/bs4ZVXXuG73/2u5ziNrfbLt83zzz+/v56kY8eOdO3alQ8++KDV/s888wzPPPPM/jk+9uzZw/r169m9ezfnnnsuBx3k1PuNHz++rM+XjQWQFPH7zjiIH2KQzXjD4vd5qarq4xa3tF1uvrB37w4aG7eg+jkiB1BVdSSdO/dos11Q57OlDuSTTz5h7Nix3HXXXVx22WUcdthh+3ML2TQ3N7e7TT6qyo033shVV13Vavntt99e0vsVI+6tsIyP/O5Jbf0isvP7vFjz4fY1NX3MZ59tQvVzAFQ/57PPNrF374422wZ9Pg866CDuuOMOfvnLX3LQQQfRv3///UOoqyqvvfZaq+0PPfTQnNuMHj2au+++2/2MTXz44Ycccsgh7N69e//+Y8eO5f7772fPnj0AbNmyhW3btvGNb3yDxx9/nE8//ZTdu3fz5JNP+vL5vCyApIjfd8Z2YcvO7/NizYfbt2/fB0BzxtJmGhvbdg8L43yeeOKJDB48mLlz5zJnzhxmz57NkCFDOP7443niiSf2b9fS8jDXNv/6r//K4sWLGTRoEMOGDWPNmjX06NGDU089lRNOOIHrr7+eMWPGcOGFF1JbW8ugQYOYOHEiu3fvZujQoZx//vkMGTKEs88+u6jJrQplw7mnyJIl/XJk3ftSW7uxpPf0s06lkth5Cddf/vIUX/3q4VnXHXJIm1HIY+Gaa65h6NChXH755VEnpZVihnO3OpAUCWJaU5ukKDs7L+ES6Zhj+QEhp6QwP/7xj1m6dGni6/ysCCtFrCjEVKpOnbrR9nLWgaqqrGOsRu6nP/0py5Yto0ePtpX8SWI5kJSxO2NTiTp2PJiqqmo+//y/222FZXIrtkrDAogxpmBxrdvp0qULH30EPXoM2l8xbYqjquzYsYMuXboUvI8FEGNyqKurS3wZtZ/iPPJA79692bx5MzbnT3m6dOlC7969C97eWmEZk4OIFJ2lr2RBtOIzyZCrFZZVopuKY7mGYNiQKiaTBRBTccqZlCqsoeWTyEYeMJksgJhIxe3CHMbQ8n7PRRHWXCE28oDJZAHERMqvKWyTknPwey6KMOcKsX5EJpNVoptIBVFR7dd7BtEKy++K6KRWbMe1ObDJLpaV6CIyTkTWicgGEZmeZf2vRGSV+3hLRHZ51jV51i0MN+WmHEnJLQSRHr8ropNYsW0zLFaOyPqBiDN4zV3AWcBmYLmILFTVNS3bqOo/era/BjjR8xafqmpNWOk1/vHe2QeRA4nzpFR+z0WRxLlCbIbFyhFlDmQksEFVG9QZxH8eMCHP9hcAc0NJmUm0uOVkvPyuiE5ixbafuaawGhCY7KIMIEcC73peb3aXtSEifYH+gHci4S4iskJEXhWRc3IdRESmuNutsF6q8RPn3EIQ/K6ITmLFtl/Nga0oLHqRVaKLyERgnKpe6b6+BDhJVadl2fYGoLeqXuNZdqSqbhGRATiBZbSqvp3vmFaJbkz0ModEASfXVGzgS2oDgiSKYyX6FuAoz+ve7rJsJpFRfKWqW9y/DUA9retHUi3IbH2ciwziVnQVt/TEhV+5piQ2IKg0UQaQ5cBAEekvzqwvk4A2ralE5GtAN2CJZ1k3Ealynx8OnAqsydw3jYLM1selyCDXhdmvPiV+iVt62hNmwOvZ8yJqazcyalQztbUbSypys57x0YssgKjqPmAa8DSwFnhUVd8UkZtEZLxn00nAPG1d1nYssEJEXgMWAzd7W2/FXZB38flauMT5vYvh54XZcglfSFrAS2IDgkoTaT8QVf2Tqh6tql9R1Vnusn9W1YWebepUdXrGfq+o6iBVHeL+nR122ksV9F18kNn6OBYZlNunxO+LZlL6uAQtjM+bxAYElcaGMglZ0HfxQWbroywyyHVhBgIfu6rYdMYpPe0JKuCFlZvxoyisXHGuFwyaBZCQBX0XH2S2PsoiAz8vzJZL+ELSAl7cxKVeMCoWQEIW9F18kNn6uBcZnH766QVtF9ZFM219XOIamJNa55gENphiyPxqA++3JE3fmiutpQyLYrMOfsHP70BczmvQv7f6+g5Ats8pjBrVXPb7x0WufiAWQCIQx5FI4/KDL0cpnyFJgTNJ4vJ9CrqzYVo6M8axI2FqxaHir1KUW2ySxuARRqVvXIrvklznmAQWQFIsrmXWxbBK4OKEVekbl/Of5DrHJLAirITyu+glLkUO5aiEzxC0SilyKbQYOK51jkljRVgVJmm9hsMQl2KTOItjZ9BiFZOLSnsOIWgWQAxQGRffuBSblCvIz5GvSCcp56/YprNW5xgcCyAJEmSdRVIuHmkQZO4yX6VvUnK1lZCLqhRWB5JQVt5fuYL+3+aqP0jKd6pS6nGSxOpATGrly13FJecVZos4b5HO009fxhFHXJyolnhpbzobJ5YDCYnfnQetA1zh8t1Zx/GuO1uawvh/x/Fc5BLHzriVzHqiE10AsaaE0aqEABJGOuN4Lkw8WBFWhNI+4FoU8hUJxb0DZakt4spNfyW0xDPhshxICNIy4FpcJS0H0qKuri5ry6iZM2f6NpikMYWIZQ5ERMaJyDoR2SAi07Osv0xEtovIKvdxpWfdZBFZ7z4mh5vy4qRl7ua43MFXChumJXppniyqEDkDiIgMEpFXReRdEblPRLp51i0r98Ai0hG4CzgbOA64QESOy7LpfFWtcR+/dfftDswETgJGAjO96YubIFqNxPGLHdd+BPmKZpJebBP34rgkS/tkUYXIlwO5G6gDBgFvAS+JyFfcdZ19OPZIYIOqNqjq58A8YEKB+44FnlXVnar6AfAsMM6HNAXC7+EU7ItdnCQ0421PrkBXqbmUOKTf6i7bly+AHKKqT6nqLlW9DZgGPCUiJ5O9QL9YRwLvel5vdpdlOk9EXheRBSJyVJH7IiJTRGSFiKzYvn27D8kujZ/DKcTpi+3XHXAcLhhxlrbzE4fcrPV4b1/eOhAR6dryXFUXA+cB/w70DThdLZ4E+qnqYJxcxkPFvoGq3qeqw1V1eHV1te8JjEK5X2w/L0Z+3QHH4YKRdEkvjoubtNRdliNfALkFONa7QFVfB0YDv/fh2FuAozyve7vLvMfboaqN7svfAsMK3beSlfvFtot1ZUp6LiVu9TnW4719OQOIqv6Hqr6aZfnfVPX7Phx7OTBQRPqLyAHAJGChdwMR6eV5OR5Y6z5/GhgjIt3cyvMx7rJUiOsXu9g74LhdMEy0oq7PyTyODQVfgJZ/UhQP4Fs4FfRvAzPcZTcB493nvwDeBF4DFgNf8+z798AG93F5IccbNmyYVor3339EX3mlry5eLPrKK331/fcfybv9zJkzFafuqtVj5syZ4SS4Hc5X0RhHFN8H+w7mBqzQLNdU60iYQnHscBbHNJnoRDHWm30Hcyu5I6GInFrIMmPKYRXAxivMYisrRi1duzkQEfkvVR3a3rIksByIw0byNaYty4HklisH0inPDrXAKUC1iFznWXUo0NH/JJqwWPAonwVhY/IXYR0AfAknyBzieXwETAw+acbElzWFrjxWjFq8Qoqw+qrqJvd5B+BLqvpRGInzmxVhGb9YcYdJk3JG4/2FiBwqIgcDq4E1InK97yk0JuaswtWY1goJIMe5OY5zgD8D/YFLAk2VMe2I4qIddUc3U5niOLJ2oQoJIJ1FpDNOAFmoqnvxZzBFY0pmdRCmEiR9ZO1CAsi9wEbgYOBFEemLU5FuTGpZhavxQ5xG1i5FuwFEVe9Q1SNV9Vtur/ZNwBkhpM14WDFJvOog7P9h/JD0IeMLaYXVE/g58GVVPdudNbBWVWeHkUA/JbkVlrX6ac3Oh6kES5b0c4uvWquq6ktt7cbwE5RDOa2wHsQZ6fbL7uu3gP/tX9LiLckVXFGxu3NjChPXkbULVUgAOVxVHwWaAVR1H9AUaKpiIuoKrjgV2RQjjApuq4MwlSDpQ8bnLMISkU6quk9E6nFmInxWVYe6U9reoqqnh5hOXxRbhFVK9nLr1jk0NMygsfFvVFX1YcCAWb58GZJUZJOktBpj2ldKEdYy9+//wZno6Ssi8jLwMHCN/0mMn2IruKLOsUQpqbklY0zp8gUQAVDVlcDpOAMrXgUcr87UthWv2Kljg2ySF/ciG+tkZ9IqzfWk+QJItYhc547Eey0wFmfq2GsyRuetWMVWcAXZJM8uxCbt4vgbSHOpA+QPIB1xRuM9JMejbCIyTkTWicgGEZmeZf11IrJGRF4XkUVuJ8aWdU0issp9LMzc1w/FVnAVm2OpVHHPLZlkiuPoA0nvCFiufJXogU4aJSIdcZoEnwVsBpYDF6jqGs82ZwBLVfUTEfkBMEpVz3fX7VHVLxVzzKD7gbTcjXi/UB06HJSoVhXGxFUcG2fU13cg+8hOwqhRzWEnJzClVKJLgOkBGAlsUNUGVf0cmAdM8G6gqotVteVq/CrQO+A0lSXpTfKMiZu4N85Ie6lDvhxId1XdGdiBRSYC41T1Svf1JcBJqjotx/a/Bt5X1Z+5r/cBq4B9wM2q+nh7x0xyT3Rj0i6OOZC0lDoUnQMJMngUS0QuBoYDt3oW93U/0IXA7SLylRz7ThGRFSKyYvv27SGktvLE5W7PmLhJe6lDIT3Rg7IFOMrzure7rBUROROYAYxX1caW5aq6xf3bANQDJ2Y7iKrep6rDVXV4dXW1f6lPoFIDQRwrL036xLVxRs+eF1Fbu5FRo5qprd2YmuABBQymCCAiR+DUWSiwXFXfL/vAIp1wKtFH4wSO5cCFqvqmZ5sTgQU4RV3rPcu7AZ+oaqOIHA4sASZ4K+CzSXsRVqlFAHEsOjDGhKfkwRRF5EqcXul/B0wEXhWRvy83Qe6YWtNwBmpcCzyqqm+KyE0iMt7d7FacpsS/y2iueyywQkReAxbj1IHkDR6mOHGvvDTGRK+Q4dzXAaeo6g73dQ/gFVU9JoT0+SqNOZC6urqsRVAzZ84sOBhYDsSYdCtnOPcdwG7P693uMpMAlTrESNLTb0wlKCSAbACWikidiMzE6Y/xlmeYE1Ph4lh5aRX7JpPdVISvkADyNvA4X3S3fAJ4Bx+HNDHhKDUQ2A/TJIHdVISvoFZYlSKNdSCVxI/6HFO5rK4uOEXXgYjI7e7fJ0VkYeYjyMQak02l1ueY0llrwWjlG8pkmKquFJGsMw+q6guBpiwAlgOpHHa3aTLZdyI4uXIgnXLt4E4klchAYSpfHCv2jUmbQjoSnioiz4rIWyLSICLviEhDGIkzJhcrojCZ7KairaBnSyykI+FfgX8EVgJNLctbOhYmiRVhGWPSws+RgsvpSPihqv5ZVbep6o6WR1FHN8YYU5Rycw9hzJaYsw5ERFpmI1wsIrcCvwe8o+H+l2+pMMYYs19m7qFlrnWg4NxDY+PfilpeipwBBPhlxmtv9kWBb/qWCmOMMfvlyz0UGkCqqvrQ2Lgp63K/5GuFdYZvRzHGGFMwP3IPAwbMyloHMmDArLLTt//92ttARH4uIod5XncTkZ/5lgJjjDGt+DHXehizJRZSiX62qu5qeaGqHwDf8i0FKRR007qwWFNaY4IxYMAsOnQ4qNWyUnIPQc+WWEgA6SgiVS0vRORAoCrP9iaPlsoxp2xS91eOJTGIlDt4nQUgY7JLylzrhfQDuQH4DvCAu+hyYKGq/kvAafNdHPqBLFnSL0fFVl9qazeGn6AylDt0hA09YUwylNwPRFVvAX6GM43sscBPkxg84sLvpnVh38Xb4HWmWPbdKEwSi7YLKcIC+AvwAlDvPveFiIwTkXUiskFEpmdZXyUi8931S0Wkn2fdje7ydSIy1q80Bc2PyjGvsOdAKHdEXAtA6WPzdLQvqUXbhbTC+h6wDJgIfA9ndsKJ5R5YRDoCdwFnA8cBF4jIcRmbXQF8oKpfBX4F3OLuexwwCTgeGAf8xn2/2POrciypbEh2Y9oKo9d4EArJgcwARqjqZFW9FBgJ/NiHY48ENqhqg6p+DswDJmRsMwF4yH2+ABgtzq3rBGCeqjaq6js40+6O9CFNgfOjciwud/FRDF5ngSYZ4vIdTYoweo0HoZBK9DdUdZDndQfgNe+ykg7s5GLGqeqV7utLgJNUdZpnm9XuNpvd128DJwF1wKuq+oi7fDbwZ1VdkOU4U4ApAH369Bm2aVPbCuwkS3JFdF1dXdEXlCR/3rQK43+2descGhpm0Nj4N6qq+jBgwKzYtVjKJ+6Na8oZTPEpEXlaRC4TkcuAPwJ/8juBQVHV+1R1uKoOr66ujjo5xsPuRo0fklp/4JXUou28AcQtLroDuBcY7D7uU9UbfDj2FuAoz+ve7rKs24hIJ6ArsKPAfVMhDXMgWHFIsgX9HU1q/YFXUvp9ZCq6CMu3AzsB4S1gNM7Ffzlwoaq+6dnmh8AgVZ0qIpOAv1PV74nI8cB/4NR7fBlYBAxU1abM43jFoR+I+UIpxQ5WhGUy1dd3wBnfNZMwalRz2MmpSOUUYf2XiIzwO0Gqug+YBjwNrAUeVdU3ReQmERnvbjYb6CEiG4DrgOnuvm8CjwJrgKeAH7YXPCpBEtuJ51IJxQ4mHvxuGm8KV+iMhF8FNgEfAwKoqg4OPnn+SnIOxM/ZxeKg1ErDUireTWWrtN9GHOXKgeSbD6RFYjrpxZUfLUT8mB8gTkpttmjBw2Rq+f7HrRVW0luGFaKQANILeFNVdwOIyKE4Q5pUVnvYgPgxs5izXzLbiecSxmQ3Jj169rwoVhdnv373cVdIHcjdwB7P6z3uMlMAv1qIVFo5b1KbLRpTiEpoGVaIQgKIqKeiRFWbKSznYvAv51BpF9ykNls0phCVVmKQSyGBoEFEruWLXMfVQENwSaosfhXVxLWctxxxK3Ywxi9pKaItJAcyFTgFp6/GZpyhRKYEmahK4mfOIcjZxSqpibAxUau0EoNcCpkPZJuqTlLV/6WqPVX1QlXdFkbiKkESimqsT4apNFG31kvC794POfuBiMg/qeq/iMidZOnmqarXBp04vyW5H0iQ4j6QmzHFshEL/FVKT/S17t8VwMosD1Mh0lLhF5Wo74aNCUrOAKKqT7p/H8r2CC+JJmiV1kQ4bmxGvnDYoJvhyxlARGRhvkeYiTTBSkuFX6HsghOcIM+tX7Nd2v+/cPmKsGpxhkn/T+A24JcZD1Mhoqjwi/OP1I8cg90NZ5eE3FgS0hgX+SrROwJnARfgzAPyR2Cud7j1pLFK9PiIcyWn32mL82cNW1jnopxBN+3/1VbRleiq2qSqT6nqZOBknHnH60VkWq59THmsL0Z2Ydy1W44hOFGc21KKrcJKYyX9zvMO5y4iVcC3cXIh/YCFwP2qmsjZ/+KcA0nDkNR1dXVZiwdmzpyZ94ca9h2h38ezIei/kIS7+yDTmNTfea4cSL4irIeBE3DmP5+nqquDTWLw4hxA0tYXo5gfadIDSBxFFdSScG6DTGNSf+el9AO5GBgI/APwioh85D52i8hHQSU0rawvRmtRFimlYZ75qCqKk3Bug0xjpf3O252RMJCDinQH5uMUi20EvqeqH2RsU4MzgOOhQBMwS1Xnu+seBE4HPnQ3v0xVV7V3XMuBxEcxd8BJuGtNmko+p3EuMkzq77ycOdGDMB1YpKoDgUXu60yfAJeq6vHAOOB2ETnMs/56Va1xH+0Gj7AVW1GWtL4Y5VYExvUHXsnS0lAgzs1wk/Y7b09UAWQC0NKb/SHgnMwNVPUtVV3vPv9vYBtQHVoKy1DK4IRJGnwt7MEXk1DskQR+dbQzpUvS77wQURVh7VLVw9znAnzQ8jrH9iNxAs3xqtrsFmHVAo24ORhVbcyx7xTc4ef79OkzbNOm4F0rAqAAABDuSURBVGfiTWo2tRB1dXWMHftgoj5fnIs0olJpRViltvAzhSm6FZYPB3wOOCLLqhnAQ96AISIfqGq3HO/TC6gHJqvqq55l7wMHAPcBb6vqTe2lKaw6kPr6DmQZwBgQRo1qDvz4QRIRFi8WkvT5Ku1i6YdKDqr2//ZfrgAS2NS0qnpmnsRsFZFeqvqeGwyyzi8iIofi9ICf0RI83Pd+z33aKCIPAD/yMellq/TZyCr986VBpQYPE66o6kAWApPd55OBJzI3EJEDgMeAh1V1Qca6Xu5fwak/iVUflSRUlBVTCZ5Z+frjH2/is89abxO3z5eWCmPTltWZhSeqOpAewKNAH2ATTjPenSIyHJiqqleKyMXAA4B37K3LVHWViDyPU6EuwCp3nz3tHTfMZrxbt86J7fzl5fSGbSkeiPPny2RFGsaUJ/Q6kDiKcz+QMJVTyZ/Ei3ES02xMnMStH4iJUDm9YZNYPJDENBuTBJYDSaFKbmZsjPGf5UDMfkmo5DfGxJ8FkBSqtN6wxphoBNYPxMRbz54XVWzASFILMVN50vT9swBiKkpmE+WWcbqAiv0Rm/hI2/fPirBMRWlomNGqfwtAc/MnNDTMiChFJk3S9v2zAGIqSntNlIPqiW493A1U3oRR7bEAYgpW7hwgYcg1HlfL8qDmiojLHBQWyKLV3vev0lgASaAoLuRhzwFSqrQ3UY5LIGuRhJsOP6Xt+2cBJGGiupAnpWw3WxPl1avHccQRF/s+sKIN2JhfUm46/JS2JvLWEz1houpFXilznAQ1LlaU423FdTIlG/GgcoQ+H4gJRlSVdEmdAySzTf7o0VGnyH/eyaHiNHBk2iqU08iKsBImqkq6JJbtZitCufHGzoEUodiAjW2lrUI5jSyAJExUF/Iklu1mq7fp2HFvIPU2can3iFMgS+JNhymO1YEkUJqGSihHpdTbJJl9VyuD1YFUkEoex8pPSa23qST2Xa1skRRhiUh3EXlWRNa7f7vl2K5JRFa5j4We5f1FZKmIbBCR+e786ca0YkUoxgQrqjqQ6cAiVR0ILHJfZ/Opqta4j/Ge5bcAv1LVrwIfAFcEm1yTREmstzEmSaIKIBOAh9znDwHnFLqjOL22vgksKGV/ky49e15Ebe1GRo1qprZ2Y+yDR1wq440pRFQBpKeqvuc+fx/omWO7LiKyQkReFZGWINED2KWq+9zXm4EjA0yrMaGJ21AkYUtqAE3bkC0tAgsgIvKciKzO8pjg3U6dZmC5moL1dWv+LwRuF5GvlJCOKW4QWrF9+/biP4gxpmyFBoYgA2hQwSmNQ7a0CCyAqOqZqnpClscTwFYR6QXg/t2W4z22uH8bgHrgRGAHcJiItLQg6w1syZOO+1R1uKoOr66u9u3zGeOXNIypFYecVVBpSMo4cUGIqghrITDZfT4ZeCJzAxHpJiJV7vPDgVOBNW6OZTEwMd/+xiRFXV0dqrp/CJKW55UUQPJJegBN85AtUQWQm4GzRGQ9cKb7GhEZLiK/dbc5FlghIq/hBIybVXWNu+4G4DoR2YBTJzI71NQbk2KFXtgLDQxBBlC/g1O2uo40D9liPdGNiRHvwIhxVcqAjYXuE+RgkOW+d+Z85+D0KzriiMm8//5DbZZXUpPxXD3RbSwsYzJE2aKmrq4u8hY9UR4/TmN5ZcpV17Fjx59S29/IAogxHlG3qInr8X/zm/PKKgrKFRgyg9UPfjDQp09SeBoKla+uI2n9jfxiRVjGeEQ9CVIxxw+iuKuQ4/tVzJSrSCiud+9RfzeiZEVYxhQg6hY1xRw/iGapYX7+pDV/tbHV2rIAYoxH1C1qknB8v+opog7WxbKx1dqyAGKMR9R3me0dP+g+E4V8fr+OFXWwLEVa6zpysQBiYiuK1kBR32W2d/ygOx2G+fmjDtamfFaJbmIpaRWsUQiyz0RYbMbCZLAZCU2i5KtgtQuMI859JgplMxYmmxVhmVhKWgVrFOLeY91UPgsgJpaSWMFqTNpYADGxZBWsxsSfBRATS1G3hjLGtM8q0U1sWQWrMfFmORBjjDElsQBijDGmJBZAjDHGlMQCiDEmMaKebMu0FkkAEZHuIvKsiKx3/3bLss0ZIrLK8/hMRM5x1z0oIu941tWE/ymMKY5d/MoT9WRbpq2ociDTgUWqOhBY5L5uRVUXq2qNqtYA3wQ+AZ7xbHJ9y3pVXRVKqo0pkV38ype0+UPSIKoAMgF4yH3+EHBOO9tPBP6sqp+0s50xsRSHi1/Sc0A2vE38RBVAeqrqe+7z94Ge7Ww/CZibsWyWiLwuIr8SkapcO4rIFBFZISIrtm/fXkaSjSld1Be/SsgB2fA28RNYABGR50RkdZbHBO926oxHnXNMahHpBQwCnvYsvhH4GjAC6A7ckGt/Vb1PVYer6vDq6upyPpIxJYv64heHHFC5bHib+AmsJ7qqnplrnYhsFZFeqvqeGyC25Xmr7wGPqepez3u35F4aReQB4Ee+JNqYgAwYMCvr/CZhXfyizgH5oWVUAps/JD6iGspkITAZuNn9+0SebS/AyXHs5wk+glN/sjqohBrjh6gvflVVfdziq7bLk8SGt4mXqALIzcCjInIFsAknl4GIDAemquqV7ut+wFHACxn7zxGRakCAVcDUcJJtTOmivPhFnQMylSmSAKKqO4DRWZavAK70vN4IHJllu28GmT5jKk3UOSBTmWw0XmNSwop/jN9sKBNjjDElsQBijDGmJBZAjDHGlMQCiDHGmJJYADHGGFMScUYSSQcR2Y7T7yQshwP/E+LximXpK4+lrzyWvvKEmb6+qtpmLKhUBZCwicgKVR0edTpysfSVx9JXHktfeeKQPivCMsYYUxILIMYYY0piASRY90WdgHZY+spj6SuPpa88kafP6kCMMcaUxHIgxhhjSmIBxBhjTEksgJRJRLqLyLMist792y3LNmeIyCrP4zMROcdd96CIvONZVxN2+tztmjxpWOhZ3l9ElorIBhGZLyIHhJ0+EakRkSUi8qaIvC4i53vWBXL+RGSciKxzP/f0LOur3POxwT0//TzrbnSXrxORsX6kp8i0XScia9xztUhE+nrWZf0/R5DGy0RkuyctV3rWTXa/D+tFZHJE6fuVJ21vicguz7pAz6GI3C8i20Qk60R54rjDTfvrIjLUsy7wc9eKqtqjjAfwL8B09/l04JZ2tu8O7AQOcl8/CEyMOn3AnhzLHwUmuc/vAX4QdvqAo4GB7vMvA+8BhwV1/oCOwNvAAOAA4DXguIxtrgbucZ9PAua7z49zt68C+rvv0zHktJ3h+X79oCVt+f7PEZy/y4BfZ9m3O9Dg/u3mPu8Wdvoytr8GuD+scwh8AxgKrM6x/lvAn3Em1DsZWBrWuct8WA6kfBOAh9znD+FMsZvPRODPqvpJO9v5pdj07SciAnwTWFDK/gVqN32q+paqrnef/zewDWjTK9ZHI4ENqtqgqp8D89x0ennTvQAY7Z6vCcA8VW1U1XeADe77hZY2VV3s+X69CvT28fi+pDGPscCzqrpTVT8AngXGRZy+C4C5PqchJ1V9EecmM5cJwMPqeBU4TER6Ec65a8UCSPl6qup77vP3gZ7tbD+Jtl/GWW5W9FciUhVR+rqIyAoRebWleA3oAexS1X3u681kmSEypPQBICIjce4a3/Ys9vv8HQm863md7XPv38Y9Px/inK9C9g06bV5X4Nyttsj2f/ZboWk8z/2/LRCRo4rcN4z04Rb/9Qee9ywO4xzmkyv9YZy7VmxGwgKIyHPAEVlWzfC+UFUVkZztot27hEHA057FN+JcOA/Aadd9A3BTBOnrq6pbRGQA8LyIvIFzUSybz+fv34HJqtrsLi77/FUqEbkYGA6c7lnc5v+sqm9nf4dAPQnMVdVGEbkKJzcXx6mqJwELVLXJsywu5zByFkAKoKpn5lonIltFpJeqvude4LbleavvAY+p6l7Pe7fcfTeKyAPAj6JIn6pucf82iEg9cCLw/3Cyx53cu+zewJYo0icihwJ/BGa42faW9y77/GWxBTjK8zrb527ZZrOIdAK6AjsK3DfotCEiZ+IE6NNVtbFleY7/s98Xv3bTqKo7PC9/i1MX1rLvqIx968NOn8ck4IfeBSGdw3xypT+Mc9eKFWGVbyHQ0tphMvBEnm3blKW6F82W+oZzgKwtL4JMn4h0ayn6EZHDgVOBNerUzC3GqbfJuX8I6TsAeAyn3HdBxrogzt9yYKA4LdAOwLmIZLa28aZ7IvC8e74WApPEaaXVHxgILPMhTQWnTUROBO4FxqvqNs/yrP9nH9NWTBp7eV6OB9a6z58Gxrhp7QaMoXWOPZT0uWn8Gk5l9BLPsrDOYT4LgUvd1lgnAx+6N1JhnLvWgqyhT8MDp9x7EbAeeA7o7i4fDvzWs10/nDuEDhn7Pw+8gXPhewT4UtjpA05x0/Ca+/cKz/4DcC6AG4DfAVURpO9iYC+wyvOoCfL84bR0eQvnznKGu+wmnIsyQBf3fGxwz88Az74z3P3WAWcH8J1rL23PAVs952phe//nCNL4C+BNNy2Lga959v1797xuAC6PIn3u6zrg5oz9Aj+HODeZ77nf+c049VhTganuegHuctP+BjA8zHPnfdhQJsYYY0piRVjGGGNKYgHEGGNMSSyAGGOMKYkFEGOMMSWxAGKMMaYkFkBMaoiIisgjntedxBkR9g/u6/GSZWRWH49fLyLDsywfLiJ3FPgePeSLkWDfF5EtntcFjZQsIqNE5JQc674mzsjHjSLiR6dMU8GsJ7pJk4+BE0TkQFX9FDgLTw9kVV1Ilg5l2bgdF0W/GFKlZKq6AlhR4LY7gBo3DXU4I8PeVuQhRwF7gFeyrNsJXIv/g2aaCmQ5EJM2fwK+7T5vNTKAOHNU/Np93lNEHhOR19zHKSLST5w5JB7G6bh4lIjcKiKrReQNaT1PyQ3ustdE5GbP8b8rIsvEmWPi6+62ozy5oDoR+Xc3F7BeRL5fyIcSkWEi8oKIrBSRpz099K+VL+YGmSfOvCVTgX90cy1f976Pqm5T1eU4ndiMyctyICZt5gH/7F6wBwP3A1/Pst0dwAuqeq6IdAS+hDOsxUCcwRxfFZHzcHIDQ4DDgeUi8qK7bAJwkqp+IiLdPe/bSVVHisi3gJlAtnHCBuPM83Aw8BcR+aM6w9hnJSKdgTuBCaq63Q1ks3B6JU8H+qszaOFhqrpLRO6htJyLMa1YADGpoqqvu3fhF+DkRnL5JnCpu08T8KE7vtAm/WIwx9NwRpRtAraKyAvACJzRbx9Qd04OVfXO7fB79+9KnOFtsnnCLWL7VEQW48xf8XietB4DnAA865Ss0RFnKAyA14E5IvJ4O+9hTNEsgJg0WgjchlMX0KPIfT8u89gtI+M2kfv3lzm+UHvjDQnwpqrWZln3bZwZ7r4DzBCRQYUm1Jj2WB2ISaP7gZ+o6ht5tlmEMx0sItJRRLpm2eY/gfPd9dU4F+plODPBXS4iB7n7d8+ybz4TRKSLiPTACXLL29l+HVAtIrXu8TqLyPEi0gE4SlUX48yT0hWnKG43cEiRaTKmDQsgJnVUdbOqttds9h+AM8SZWGslzlznmR7DKSJ6DWdU4H9S1fdV9SmcXM4KEVlF8XOUvI4zQu2rwE/z1X8AqDMt60TgFhF5DWcE3lNwirIecT/DX4A7VHUXzmRO52arRBeRI0RkM3Ad8H9FZLM4c7EY04aNxmtMjJTRNNeY0FkOxBhjTEksB2KMMaYklgMxxhhTEgsgxhhjSmIBxBhjTEksgBhjjCmJBRBjjDEl+f8u9aQ2X5kREwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot examples\n",
        "plot_data(X_train, y_train[:], pos_label=\"Accepted\", neg_label=\"Rejected\")\n",
        "\n",
        "# Set the y-axis label\n",
        "plt.ylabel('Microchip Test 2') \n",
        "# Set the x-axis label\n",
        "plt.xlabel('Microchip Test 1') \n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2RxirWyyhR0"
      },
      "source": [
        "Figure shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8mMNEG4yhR1"
      },
      "source": [
        "# **Feature mapping**\n",
        "\n",
        "One way to fit the data better is to create more features from each data point. In the provided function `map_feature`, we will map the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power.\n",
        "\n",
        "$$\\mathrm{map\\_feature}(x) = \n",
        "\\left[\\begin{array}{c}\n",
        "x_1\\\\\n",
        "x_2\\\\\n",
        "x_1^2\\\\\n",
        "x_1 x_2\\\\\n",
        "x_2^2\\\\\n",
        "x_1^3\\\\\n",
        "\\vdots\\\\\n",
        "x_1 x_2^5\\\\\n",
        "x_2^6\\end{array}\\right]$$\n",
        "\n",
        "As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 27-dimensional vector. \n",
        "\n",
        "- A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will be nonlinear when drawn in our 2-dimensional plot.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_feature(X1, X2):\n",
        "    \"\"\"\n",
        "    Feature mapping function to polynomial features    \n",
        "    \"\"\"\n",
        "    X1 = np.atleast_1d(X1)\n",
        "    X2 = np.atleast_1d(X2)\n",
        "    degree = 6\n",
        "    out = []\n",
        "    for i in range(1, degree+1):\n",
        "        for j in range(i + 1):\n",
        "            out.append((X1**(i-j) * (X2**j)))\n",
        "    return np.stack(out, axis=1)\n"
      ],
      "metadata": {
        "id": "QTG8xbCa6PhE"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnBHFKxSyhR1",
        "outputId": "3f322f75-3c3e-44e2-fd0c-5cf10fafd67b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape of data: (118, 2)\n",
            "Shape after feature mapping: (118, 27)\n"
          ]
        }
      ],
      "source": [
        "print(\"Original shape of data:\", X_train.shape)\n",
        "\n",
        "mapped_X =  map_feature(X_train[:, 0], X_train[:, 1])\n",
        "print(\"Shape after feature mapping:\", mapped_X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fba_VWs6yhR2"
      },
      "source": [
        "Let's also print the first elements of `X_train` and `mapped_X` to see the tranformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I5_DXzmyhR2",
        "outputId": "a82505bc-736f-465d-cd89-1316f84f73b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train[0]: [0.051267 0.69956 ]\n",
            "mapped X_train[0]: [5.12670000e-02 6.99560000e-01 2.62830529e-03 3.58643425e-02\n",
            " 4.89384194e-01 1.34745327e-04 1.83865725e-03 2.50892595e-02\n",
            " 3.42353606e-01 6.90798869e-06 9.42624411e-05 1.28625106e-03\n",
            " 1.75514423e-02 2.39496889e-01 3.54151856e-07 4.83255257e-06\n",
            " 6.59422333e-05 8.99809795e-04 1.22782870e-02 1.67542444e-01\n",
            " 1.81563032e-08 2.47750473e-07 3.38066048e-06 4.61305487e-05\n",
            " 6.29470940e-04 8.58939846e-03 1.17205992e-01]\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train[0]:\", X_train[0])\n",
        "print(\"mapped X_train[0]:\", mapped_X[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLfr101ByhR3"
      },
      "source": [
        "# **Cost function for regularized logistic regression**\n",
        "\n",
        "In this part, we will implement the cost function for regularized logistic regression.\n",
        "\n",
        "Recall that for regularized logistic regression, the cost function is of the form\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
        "\n",
        "Compare this to the cost function without regularization, which is of the form \n",
        "\n",
        "$$ J(\\mathbf{w}.b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right]$$\n",
        "\n",
        "The difference is the regularization term, which is $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$ \n",
        "Note that the $b$ parameter is not regularized."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "\n",
        "    g = 1/(1+np.exp(-z))\n",
        "    \n",
        "    return g"
      ],
      "metadata": {
        "id": "We9PJ87F_Fxv"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, w, b, lambda_= 1):\n",
        "\n",
        "    m, n = X.shape\n",
        "    \n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        z_i = np.dot(X[i],w) + b\n",
        "        f_wb_i = sigmoid(z_i)\n",
        "        cost += -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
        "    total_cost = cost/m \n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "HLvENvHR-6wW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrqbfICwyhR3"
      },
      "source": [
        "Please complete the `compute_cost_reg` function below to calculate the following term for each element in $w$ \n",
        "$$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
        "\n",
        "The starter code then adds this to the cost without regularization (which you computed above in `compute_cost`) to calculate the cost with regulatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "gqZu3L3lyhR3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
        "\n",
        "    m, n = X.shape\n",
        "    \n",
        "    # Calls the compute_cost function that you implemented above\n",
        "    cost_without_reg = compute_cost(X, y, w, b) \n",
        "    \n",
        "    # We need to calculate this value\n",
        "    reg_cost = 0.\n",
        "    \n",
        "    for j in range(n):\n",
        "        reg_cost += (w[j]**2)\n",
        "    \n",
        "    \n",
        "    # Add the regularization cost to get the total cost\n",
        "    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost\n",
        "\n",
        "    return total_cost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the cell below to check the implementation of the compute_cost_reg function.**"
      ],
      "metadata": {
        "id": "CKdxgofUAEee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
        "np.random.seed(1)\n",
        "initial_w = np.random.rand(X_mapped.shape[1]) - 0.5\n",
        "initial_b = 0.5\n",
        "lambda_ = 0.5\n",
        "cost = compute_cost_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
        "\n",
        "print(\"Regularized cost :\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l25-liNG_xx5",
        "outputId": "0514494b-740d-4311-caef-8efd21fbb467"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regularized cost : 0.6618252552483948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn3u0GrLyhR5"
      },
      "source": [
        "\n",
        "# **Gradient for regularized logistic regression**\n",
        "\n",
        "In this section, we will implement the gradient for regularized logistic regression.\n",
        "\n",
        "\n",
        "The gradient of the regularized cost function has two components. The first, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ is a scalar, the other is a vector with the same shape as the parameters $\\mathbf{w}$, where the $j^\\mathrm{th}$ element is defined as follows:\n",
        "\n",
        "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})  $$\n",
        "\n",
        "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$\n",
        "\n",
        "Compare this to the gradient of the cost function without regularization (which you implemented above), which is of the form \n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
        "$$\n",
        "\n",
        "\n",
        "As we can see,$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ is the same, the difference is the following term in $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, which is $$\\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$ \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9p7V2qPyhR6"
      },
      "source": [
        "We have to complete the `compute_gradient_reg` function below to modify the code below to calculate the following term\n",
        "\n",
        "$$\\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$\n",
        "\n",
        "The starter code will add this term to the $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$ returned from `compute_gradient` above to get the gradient for the regularized cost function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b, lambda_=None): \n",
        "\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i],w) + b)\n",
        "        err_i = f_wb_i - y[i]\n",
        "        for j in range(n): \n",
        "            dj_dw[j] += err_i*X[i,j]\n",
        "        \n",
        "        dj_db += err_i\n",
        "        \n",
        "    dj_dw = dj_dw/m\n",
        "    dj_db = dj_db/m\n",
        "    \n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "LeHUGpUTAqfk"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "zcM_famcyhR6"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
        "    m, n = X.shape\n",
        "    \n",
        "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
        "     \n",
        "    for j in range(n):\n",
        "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]         \n",
        "        \n",
        "    return dj_db, dj_dw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JpyQ5NeyhR_"
      },
      "source": [
        "\n",
        "### **Learning parameters using gradient descent**\n",
        "We will use our gradient descent function implemented above to learn the optimal parameters $w$,$b$. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
        "    \n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "    \n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w_in = w_in - alpha * dj_dw               \n",
        "        b_in = b_in - alpha * dj_db              \n",
        "       \n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion \n",
        "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
        "            w_history.append(w_in)\n",
        "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
        "        \n",
        "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "fsAkPd4kBQs9"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqnlwbFHyhR_",
        "outputId": "2cab3d26-9410-4a5b-d70b-4664e3bed59b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0: Cost     0.72   \n",
            "Iteration 1000: Cost     0.59   \n",
            "Iteration 2000: Cost     0.56   \n",
            "Iteration 3000: Cost     0.53   \n",
            "Iteration 4000: Cost     0.51   \n",
            "Iteration 5000: Cost     0.50   \n",
            "Iteration 6000: Cost     0.48   \n",
            "Iteration 7000: Cost     0.47   \n",
            "Iteration 8000: Cost     0.46   \n",
            "Iteration 9000: Cost     0.45   \n",
            "Iteration 9999: Cost     0.45   \n"
          ]
        }
      ],
      "source": [
        "# Initialize fitting parameters\n",
        "np.random.seed(1)\n",
        "initial_w = np.random.rand(X_mapped.shape[1])-0.5\n",
        "initial_b = 1.\n",
        "\n",
        "# Set regularization parameter lambda_ to 1 (we can try varying this)\n",
        "lambda_ = 0.01;                                          \n",
        "# Some gradient descent settings\n",
        "iterations = 10000\n",
        "alpha = 0.01\n",
        "\n",
        "w,b, J_history,_ = gradient_descent(X_mapped, y_train, initial_w, initial_b, \n",
        "                                    compute_cost_reg, compute_gradient_reg, \n",
        "                                    alpha, iterations, lambda_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOWkAzMEyhSB"
      },
      "source": [
        "### **Evaluating regularized logistic regression model**\n",
        "\n",
        "We will use the `predict` function that we implemented above to calculate the accuracy of the regulaized logistic regression model on the training set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b): \n",
        "    \n",
        "    # number of training examples\n",
        "    m, n = X.shape   \n",
        "    p = np.zeros(m)\n",
        "   \n",
        "    \n",
        "    # Loop over each example\n",
        "    for i in range(m):   \n",
        "        z_wb = 0\n",
        "        # Loop over each feature\n",
        "        for j in range(n): \n",
        "            # Add the corresponding term to z_wb\n",
        "            z_wb_ij = X[i, j] * w[j]\n",
        "            z_wb += z_wb_ij\n",
        "        \n",
        "        # Add bias term \n",
        "        z_wb += b\n",
        "        \n",
        "        # Calculate the prediction for this example\n",
        "        f_wb = sigmoid(z_wb)\n",
        "\n",
        "        # Apply the threshold\n",
        "        if f_wb >= 0.5:\n",
        "            p[i] = 1\n",
        "        else:\n",
        "            p[i] = 0\n",
        "        \n",
        "    return p"
      ],
      "metadata": {
        "id": "K-AL5csrB8wI"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46CdXvvYyhSB",
        "outputId": "18034604-19d6-4592-c82e-8d29f4845889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 82.203390\n"
          ]
        }
      ],
      "source": [
        "#Compute accuracy on the training set\n",
        "p = predict(X_mapped, w, b)\n",
        "\n",
        "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}